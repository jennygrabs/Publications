
@inproceedings{leipzig_biodiversity_2021,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Biodiversity {Image} {Quality} {Metadata} {Augments} {Convolutional} {Neural} {Network} {Classification} of {Fish} {Species}},
	isbn = {978-3-030-71903-6},
	doi = {10.1007/978-3-030-71903-6_1},
	abstract = {Biodiversity image repositories are crucial sources for training machine learning approaches to support biological research. Metadata about object (e.g. image) quality is a putatively important prerequisite to selecting samples for these experiments. This paper reports on a study demonstrating the importance of image quality metadata for a species classification experiment involving a corpus of 1935 fish specimen images which were annotated with 22 metadata quality properties. A small subset of high quality images produced an F1 accuracy of 0.41 compared to 0.35 for a taxonomically matched subset low quality images when used by a convolutional neural network approach to species identification. Using the full corpus of images revealed that image quality differed between correctly classified and misclassified images. We found anatomical feature visibility was the most important quality feature for classification accuracy. We suggest biodiversity image repositories consider adopting a minimal set of image quality metadata to support machine learning.},
	language = {en},
	booktitle = {Metadata and {Semantic} {Research}},
	publisher = {Springer International Publishing},
	author = {Leipzig, Jeremy and Bakis, Yasin and Wang, Xiaojun and Elhamod, Mohannad and Diamond, Kelly and Dahdul, Wasila and Karpatne, Anuj and Maga, Murat and Mabee, Paula and Bart, Henry L. and Greenberg, Jane},
	editor = {Garoufallou, Emmanouel and Ovalle-Perandones, María-Antonia},
	year = {2021},
	keywords = {Convolutional neural networks, Image classification, Image metadata, Quality metadata},
	pages = {3--12},
	file = {Submitted Version:C\:\\Users\\murat\\Zotero\\storage\\VBXXPPYQ\\Leipzig et al. - 2021 - Biodiversity Image Quality Metadata Augments Convo.pdf:application/pdf},
}

@article{diamond_computational_2022,
	title = {Computational anatomy and geometric shape analysis enables analysis of complex craniofacial phenotypes in zebrafish},
	volume = {11},
	issn = {2046-6390},
	url = {https://doi.org/10.1242/bio.058948},
	doi = {10.1242/bio.058948},
	abstract = {Due to the complexity of fish skulls, previous attempts to classify craniofacial phenotypes have relied on qualitative features or sparce 2D landmarks. In this work we aim to identify previously unknown 3D craniofacial phenotypes with a semiautomated pipeline in adult zebrafish mutants. We first estimate a synthetic ‘normative’ zebrafish template using MicroCT scans from a sample pool of wild-type animals using the Advanced Normalization Tools (ANTs). We apply a computational anatomy (CA) approach to quantify the phenotype of zebrafish with disruptions in bmp1a, a gene implicated in later skeletal development and whose human ortholog when disrupted is associated with Osteogenesis Imperfecta. Compared to controls, the bmp1a fish have larger otoliths, larger normalized centroid sizes, and exhibit shape differences concentrated around the operculum, anterior frontal, and posterior parietal bones. Moreover, bmp1a fish differ in the degree of asymmetry. Our CA approach offers a potential pipeline for high-throughput screening of complex fish craniofacial shape to discover novel phenotypes for which traditional landmarks are too sparce to detect. The current pipeline successfully identifies areas of variation in zebrafish mutants, which are an important model system for testing genome to phenome relationships in the study of development, evolution, and human diseases.This article has an associated First Person interview with the first author of the paper.},
	number = {2},
	urldate = {2022-02-25},
	journal = {Biology Open},
	author = {Diamond, Kelly M. and Rolfe, Sara M. and Kwon, Ronald Y. and Maga, A. Murat},
	month = feb,
	year = {2022},
	pages = {bio058948},
	file = {Full Text PDF:C\:\\Users\\murat\\Zotero\\storage\\PK5AUHJX\\Diamond et al. - 2022 - Computational anatomy and geometric shape analysis.pdf:application/pdf;Snapshot:C\:\\Users\\murat\\Zotero\\storage\\WD76YL7D\\Computational-anatomy-and-geometric-shape-analysis.html:text/html},
}

@article{elhamod_hierarchy-guided_2022,
	title = {Hierarchy-guided neural network for species classification},
	volume = {13},
	issn = {2041-210X},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13768},
	doi = {10.1111/2041-210X.13768},
	abstract = {Species classification is an important task which is the foundation of industrial, commercial, ecological and scientific applications involving the study of species distributions, dynamics and evolution. While conventional approaches for this task use off-the-shelf machine learning (ML) methods such as existing Convolutional Neural Network (ConvNet) architectures, there is an opportunity to inform the ConvNet architecture using our knowledge of biological hierarchies among taxonomic classes. In this work, we propose a new approach for species classification termed hierarchy-guided neural network (HGNN), which infuses hierarchical taxonomic information into the neural network's training to guide the structure and relationships among the extracted features. We perform extensive experiments on an illustrative use-case of classifying fish species to demonstrate that HGNN outperforms conventional ConvNet models in terms of classification accuracy, especially under scarce training data conditions. We also observe that HGNN shows better resilience to adversarial occlusions, when some of the most informative patch regions of the image are intentionally blocked and their effect on classification accuracy is studied.},
	language = {en},
	number = {3},
	urldate = {2022-03-28},
	journal = {Methods in Ecology and Evolution},
	author = {Elhamod, Mohannad and Diamond, Kelly M. and Maga, A. Murat and Bakis, Yasin and Bart Jr., Henry L. and Mabee, Paula and Dahdul, Wasila and Leipzig, Jeremy and Greenberg, Jane and Avants, Brian and Karpatne, Anuj},
	year = {2022},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13768},
	keywords = {neural networks, machine learning, science guided machine learning, species classification, taxonomy},
	pages = {642--652},
	file = {Full Text PDF:C\:\\Users\\murat\\Zotero\\storage\\HABEPSSZ\\Elhamod et al. - 2022 - Hierarchy-guided neural network for species classi.pdf:application/pdf;Snapshot:C\:\\Users\\murat\\Zotero\\storage\\XCL55L8T\\2041-210X.html:text/html},
}


@ARTICLE{Song2022-wc,
  title         = "One Step at a Time: {Long-Horizon} {Vision-and-Language}
                   Navigation with Milestones",
  author        = "Song, Chan Hee and Kil, Jihyung and Pan, Tai-Yu and Sadler,
                   Brian M and Chao, Wei-Lun and Su, Yu",
  abstract      = "We study the problem of developing autonomous agents that
                   can follow human instructions to infer and perform a
                   sequence of actions to complete the underlying task.
                   Significant progress has been made in recent years,
                   especially for tasks with short horizons. However, when it
                   comes to long-horizon tasks with extended sequences of
                   actions, an agent can easily ignore some instructions or get
                   stuck in the middle of the long instructions and eventually
                   fail the task. To address this challenge, we propose a
                   model-agnostic milestone-based task tracker (M-TRACK) to
                   guide the agent and monitor its progress. Specifically, we
                   propose a milestone builder that tags the instructions with
                   navigation and interaction milestones which the agent needs
                   to complete step by step, and a milestone checker that
                   systemically checks the agent's progress in its current
                   milestone and determines when to proceed to the next. On the
                   challenging ALFRED dataset, our M-TRACK leads to a notable
                   33\% and 52\% relative improvement in unseen success rate
                   over two competitive base models.",
  month         =  feb,
  year          =  2022,
  url           = "http://arxiv.org/abs/2202.07028",
  archivePrefix = "arXiv",
  eprint        = "2202.07028",
  primaryClass  = "cs.AI",
  arxivid       = "2202.07028"
}

